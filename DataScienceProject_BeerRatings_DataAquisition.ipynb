{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import the relevant modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inital data set was provided on Kaggle collected by jtrofe (https://www.kaggle.com/jtrofe/beer-recipes) and using data from www.Brewersfriend.com. The initial data provided in this data set was a robust starting point for this project. However, additional data was acquired to supplement this inital set of 73,800+ entries. The original data came in two .csv files. The first (recipeData.csv) contains most of the information on the homebrews. The second (styleData.csv) contains the assignment for the styles of beer found in the recipeData file.\n",
    "\n",
    "The data acqusitition for my project is in two parts. The first part is scraping ratings data from the website using the Beautiful Soup package. The second part is to use the API to obtain recipe data (e.g. ingredients, hops, yeast, etc.) for each entry.\n",
    "\n",
    "A few separate files used in this notebook have been augmented from the original data set. The first (recipeData_urls_all.csv) only contains all of the urls in the original recipeData.csv document. These entries specifically are the subdirectories pertaining to each beer in the dataset. The second (recipe_id.csv) is generated below in part two and contains a list of each of the recipe id harvested from the urls in the original recipeData.csv document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Data Acquisition of Ratings Data via Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish headers, the base_url (or domain), and a list to accept data from Beautiful Soup\n",
    "\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.108 Safari/537.36'\n",
    "}\n",
    "\n",
    "base_url = 'https://www.brewersfriend.com'\n",
    "data = [[\"url\", \"rating\", \"reviews\", \"calories\", \"carbs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to help sift through the soup: ladel\n",
    "\n",
    "def ladle(url):\n",
    "    # This function takes the url given and requests the rating, review, calories, and carbs of the beer in question.\n",
    "    # The entry is appended to the defined list.\n",
    "    \n",
    "    beer_html = requests.get(url, headers=headers).text\n",
    "\n",
    "    soup = BeautifulSoup(beer_html, 'html5lib')\n",
    "\n",
    "    rating = soup.find('span', {'itemprop': 'ratingValue'}) if soup.find('span', {'itemprop': 'ratingValue'}) != None else \"NaN\"\n",
    "    review = soup.find('span', {'itemprop': 'reviewCount'}) if soup.find('span', {'itemprop': 'reviewCount'}) != None else \"NaN\"\n",
    "    calories = soup.find('strong', {'class': 'calories'}) if soup.find('strong', {'class': 'calories'}) != None else \"NaN\"\n",
    "    carbs = soup.find('strong', {'class': 'carbs'}) if soup.find('strong', {'class': 'carbs'}) != None else \"NaN\"\n",
    "\n",
    "    temp = [url, \n",
    "            rating.text if rating != \"NaN\" else 'NaN', \n",
    "            review.text if review != \"NaN\" else 'NaN', \n",
    "            calories.text if calories != \"NaN\" else 'NaN', \n",
    "            carbs.text if carbs != \"NaN\" else 'NaN']\n",
    "    data.append(temp)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0\n",
      "73851  https://www.brewersfriend.com/homebrew/recipe/...\n",
      "73852  https://www.brewersfriend.com/homebrew/recipe/...\n",
      "73853  https://www.brewersfriend.com/homebrew/recipe/...\n",
      "73854  https://www.brewersfriend.com/homebrew/recipe/...\n",
      "73855  https://www.brewersfriend.com/homebrew/recipe/...\n",
      "73856  https://www.brewersfriend.com/homebrew/recipe/...\n",
      "73857  https://www.brewersfriend.com/homebrew/recipe/...\n",
      "73858  https://www.brewersfriend.com/homebrew/recipe/...\n",
      "73859  https://www.brewersfriend.com/homebrew/recipe/...\n",
      "73860  https://www.brewersfriend.com/homebrew/recipe/...\n"
     ]
    }
   ],
   "source": [
    "# A dataframe of the full url is generated for each url fragment in Kaggle data set.\n",
    "\n",
    "url_df = pd.read_csv('recipeData_urls_all.csv', header= None, engine = 'python', encoding = 'ISO-8859-1').apply(lambda x: base_url + x)\n",
    "\n",
    "print(url_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a for loop: for each full url in the dataframe \"url_df\", perform the ladel function. This will build up a list: data\n",
    "\n",
    "for index, row in url_df.iterrows():\n",
    "    ladle(row[0])\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      0    1    2  \\\n",
      "3433  https://www.brewersfriend.com/homebrew/recipe/...  NaN  NaN   \n",
      "3434  https://www.brewersfriend.com/homebrew/recipe/...  NaN  NaN   \n",
      "3435  https://www.brewersfriend.com/homebrew/recipe/...  NaN  NaN   \n",
      "3436  https://www.brewersfriend.com/homebrew/recipe/...  NaN  NaN   \n",
      "3437  https://www.brewersfriend.com/homebrew/recipe/...  NaN  NaN   \n",
      "\n",
      "                 3       4  \n",
      "3433           NaN     NaN  \n",
      "3434  159 calories  17.2 g  \n",
      "3435  153 calories  14.8 g  \n",
      "3436  206 calories  22.4 g  \n",
      "3437  435 calories  29.3 g  \n"
     ]
    }
   ],
   "source": [
    "# Convert the list \"data\" into a dataframe: df\n",
    "# This step took many hours to complete and was done in a separate Jupyter notebook with the same code.\n",
    "# A sample tail output was generated below from a failed attempt.\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the webscraped data as a csv file for later: reviewData_all.csv\n",
    "\n",
    "df.to_csv('reviewData_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From step one, we have available review data from most of the entries. This was limited by two factors. \n",
    "\n",
    "The first limitation was from the data itself. Only entries that have reviews and ratings at the time of scraping will be included.\n",
    "\n",
    "The second limitation is the time required for this scraping to occur. Due to how large the data set is, I experienced problems\n",
    "scraping due to TimeOut Errors with the server. This was possibly due to over-requesting and being rejected from the server end to \n",
    "avoid a crash. Another possibility was user error on my end with incorrect settings on my computer that would interupt the request.\n",
    "AS SUCH, ONLY A PORTION OF THE DATA SET WAS PROPERLY SCRAPED OVER MANY HOURS. One optimization for time management is to use \n",
    "dataframe methods instead of list methods for this harvesting which may speed up the process.\n",
    "\n",
    "Going forward, I will use the best harvested review data csv I was able to get for the next steps: 'reviewData_16822.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Data Acquisition of Ingredient Data via Brewer's Friend API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the recipe ID numbers need to be extracted from the recipeData.csv original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0\n",
      "73851          /homebrew/recipe/view/615556/blonde-stout\n",
      "73852        /homebrew/recipe/view/618629/session-simcoe\n",
      "73853  /homebrew/recipe/view/602248/chris-ford-wheat-ipa\n",
      "73854  /homebrew/recipe/view/603016/x-files-american-ale\n",
      "73855           /homebrew/recipe/view/607368/unicorn-pee\n",
      "73856         /homebrew/recipe/view/609673/amber-alfie-2\n",
      "73857               /homebrew/recipe/view/610955/rye-ipa\n",
      "73858                      /homebrew/recipe/view/586891/\n",
      "73859                      /homebrew/recipe/view/603788/\n",
      "73860  /homebrew/recipe/view/613776/elvis-juice-ipa-c...\n"
     ]
    }
   ],
   "source": [
    "url_df = pd.read_csv('recipeData_urls.csv', header= None, engine = 'python', encoding = 'ISO-8859-1')\n",
    "\n",
    "print(url_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0         1       2     3       4                      5\n",
      "73851    homebrew  recipe  view  615556           blonde-stout\n",
      "73852    homebrew  recipe  view  618629         session-simcoe\n",
      "73853    homebrew  recipe  view  602248   chris-ford-wheat-ipa\n",
      "73854    homebrew  recipe  view  603016   x-files-american-ale\n",
      "73855    homebrew  recipe  view  607368            unicorn-pee\n",
      "73856    homebrew  recipe  view  609673          amber-alfie-2\n",
      "73857    homebrew  recipe  view  610955                rye-ipa\n",
      "73858    homebrew  recipe  view  586891                       \n",
      "73859    homebrew  recipe  view  603788                       \n",
      "73860    homebrew  recipe  view  613776  elvis-juice-ipa-clone\n"
     ]
    }
   ],
   "source": [
    "new_list = []\n",
    "for index,row in url_df.iterrows():\n",
    "    temp = row[0].rsplit('/')\n",
    "    new_list.append(temp)\n",
    "    \n",
    "rec_df = pd.DataFrame(new_list)\n",
    "print(rec_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       4\n",
      "0   1633\n",
      "1  16367\n",
      "2   5920\n",
      "3   5916\n",
      "4  89534\n"
     ]
    }
   ],
   "source": [
    "# Getting only the column with the recipe ID's, \n",
    "\n",
    "rec_id = rec_df[[4]]\n",
    "print(rec_id.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of recipe_IDs is saved for future reference: 'recipe_id.csv'\n",
    "rec_id.to_csv('recipe_id.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the newly created list of recipe IDs, rec_id, the ingredient list API can now be set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish new headers and base url (domain) for the recipe and ingredients\n",
    "\n",
    "my_headers = {'X-API-KEY': '1062c1a3650672bb65e9dc8c71bd7dfe4061166f'}\n",
    "base_rec_url = 'https://api.brewersfriend.com/v1/recipes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create five empty dataframes, one for each category of ingredient and place in a dictionary: ingredient_dict\n",
    "\n",
    "fermentables = pd.DataFrame()\n",
    "hops = pd.DataFrame()\n",
    "misc = pd.DataFrame()\n",
    "mash = pd.DataFrame()\n",
    "yeast = pd.DataFrame()\n",
    "\n",
    "ingredient_dict = {\"FERMENTABLE\": fermentables, \"HOP\": hops, \"MISC\": misc, 'MASH': mash,'YEAST': yeast}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to help parse through xml data for each recipe: xml_sift()\n",
    "\n",
    "def xml_sift(xml_file, recipe_id, xpath_loc, df):\n",
    "    \n",
    "    \"\"\" This function looks at the xml data from Brewer's Friend recipe API output, specifically looking at the Fermentables, Hops, \n",
    "    Misc, Mash Steps, and Yeast used and adds the value of Recipe ID in a new column. The MASH portion of the if-else statement is \n",
    "    needed because the xpath is unique compared to the other ingredients. The try-except clause is required to ignore recipes that \n",
    "    do not contain a given ingredient type (usually MISC values are missing).\"\"\" \n",
    "    \n",
    "    if 'MASH' in xpath_loc:\n",
    "        try:\n",
    "            temp = pd.read_xml(xml_file, xpath = \"/RECIPES/RECIPE/MASH/MASH_STEPS/MASH_STEP\")\n",
    "            temp.insert(0, \"Recipe_ID\", recipe_id, True)\n",
    "            return temp\n",
    "        except ValueError: \n",
    "            pass \n",
    "    else:\n",
    "        try:\n",
    "            temp = pd.read_xml(xml_file, xpath = \"/RECIPES/RECIPE/\" + xpath_loc + 'S/' + xpath_loc)\n",
    "            temp.insert(0, \"Recipe_ID\", recipe_id, True)\n",
    "            return temp\n",
    "        except ValueError: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get ingredient data for a given recipe: gather_ingredients()\n",
    "\n",
    "def gather_ingredients(recipe_id):\n",
    "    \n",
    "    \"\"\"This function uses the domain (base_rec_url) and the input recipe_id to request an xml using the API from\n",
    "    Brewer's Friend (https://docs.brewersfriend.com/api/recipes). This function uses standard request.get to retrieve\n",
    "    the xml file. This file cannot be converted into a json. The for loop works through each of the five ingredient\n",
    "    types using the dictionary ingredient_dict. The results from xml_sift are concatenated onto the respective dataframe\n",
    "    by adding rows under the previous recipe (axis = 0). \n",
    "    \n",
    "    The subfunction '.reset_index()' was required to get this to work because otherwise we were rewriting index 0-5 multiple times.\n",
    "    The try-except clause is used to acknowledge missing entries or xpaths passed from the previous function (e.g.\n",
    "    entries that don't have MISC ingredients listed will be ignored).\"\"\"\n",
    "    \n",
    "    url = base_rec_url + str(recipe_id) + '.xml'\n",
    "\n",
    "    r = requests.get(url, headers=my_headers)\n",
    "\n",
    "    xml_file = r.text\n",
    "    \n",
    "    for ingredient in ingredient_dict:\n",
    "        try: \n",
    "            ingredient_dict[ingredient] = pd.concat([ingredient_dict[ingredient], xml_sift(xml_file, recipe_id, ingredient, ingredient_dict[ingredient])], axis = 0, ignore_index = True, sort = False).reset_index(drop=True)\n",
    "        except:\n",
    "            continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a for loop: for each recipe ID in the dataframe \"rec_id\", perform the gather_ingredients function. \n",
    "\n",
    "for index, row in rec_id.iterrows():\n",
    "    gather_ingredients(row[4])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294096, 10)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the FERMENTABLE dataframe \n",
    "\n",
    "print(ingredient_dict['FERMENTABLE'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each of the dataframes in 'ingredient_dict' as a csv file\n",
    "\n",
    "for k,v in ingredient_dict.items():\n",
    "    ingredient_dict[k].to_csv(path_or_buf = k+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From step two, we have five CSV files that each contain all of the ingredient data for each recipe saved (e.g. 'FERMENTABLE.CSV'). \n",
    "\n",
    "In the data wrangling component, using the six CSV files gathered here and the original data set, we'll go through and \n",
    "design the final data set that will be used for modeling. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
